"""
ç¬¬ä¸€è¯¾ï¼šLangChain åŸºç¡€ - ä½¿ç”¨ç»Ÿä¸€å…¼å®¹å±‚è¿›è¡Œå¯¹è¯
å­¦ä¹ ç›®æ ‡ï¼š
1. äº†è§£ LangChain çš„åŸºæœ¬æ¦‚å¿µ
2. ä½¿ç”¨ç»Ÿä¸€çš„ LLM å·¥å‚åˆ‡æ¢ä¸åŒæ¨¡å‹
3. æŒæ¡ 4 ç§æ¶ˆæ¯æ ¼å¼
4. æŒæ¡æ‰€æœ‰è°ƒç”¨æ–¹å¼ï¼šinvoke/stream/batch åŠå…¶å¼‚æ­¥ç‰ˆæœ¬
"""
import asyncio
from langchain_core.messages import HumanMessage, SystemMessage
from llm_factory import get_llm, list_providers

# ä½¿ç”¨ç»Ÿä¸€å·¥å‚è·å– LLM
llm = get_llm()


# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼š4 ç§æ¶ˆæ¯æ ¼å¼
# ============================================================

def demo_message_formats():
    """æ¼”ç¤º 4 ç§æ¶ˆæ¯è¾“å…¥æ ¼å¼"""
    print("=" * 50)
    print("ğŸ“ 4 ç§æ¶ˆæ¯æ ¼å¼æ¼”ç¤º")
    print("=" * 50)
    
    # æ–¹å¼1ï¼šçº¯å­—ç¬¦ä¸²ï¼ˆæœ€ç®€å•ï¼‰
    print("\n1ï¸âƒ£ å­—ç¬¦ä¸²æ ¼å¼ï¼ˆæœ€ç®€å•ï¼‰")
    response = llm.invoke("ç”¨ä¸€å¥è¯ä»‹ç»Python")
    print(f"   å“åº”: {response.content}")
    
    # æ–¹å¼2ï¼šå…ƒç»„åˆ—è¡¨ï¼ˆæ¨èï¼Œç®€æ´æ˜“è¯»ï¼‰
    print("\n2ï¸âƒ£ å…ƒç»„åˆ—è¡¨æ ¼å¼ï¼ˆæ¨èï¼‰")
    response = llm.invoke([
        ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œç”¨ä¸€å¥è¯å›ç­”"),
        ("human", "ä»€ä¹ˆæ˜¯Javaï¼Ÿ"),
    ])
    print(f"   å“åº”: {response.content}")
    
    # æ–¹å¼3ï¼šå­—å…¸åˆ—è¡¨ï¼ˆOpenAI åŸç”Ÿæ ¼å¼ï¼‰
    print("\n3ï¸âƒ£ å­—å…¸åˆ—è¡¨æ ¼å¼ï¼ˆOpenAIå…¼å®¹ï¼‰")
    response = llm.invoke([
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œç”¨ä¸€å¥è¯å›ç­”"},
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯Goè¯­è¨€ï¼Ÿ"},
    ])
    print(f"   å“åº”: {response.content}")
    
    # æ–¹å¼4ï¼šMessage å¯¹è±¡ï¼ˆå®Œæ•´æ§åˆ¶ï¼‰
    print("\n4ï¸âƒ£ Messageå¯¹è±¡æ ¼å¼ï¼ˆå®Œæ•´æ§åˆ¶ï¼‰")
    response = llm.invoke([
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œç”¨ä¸€å¥è¯å›ç­”"),
        HumanMessage(content="ä»€ä¹ˆæ˜¯Rustï¼Ÿ"),
    ])
    print(f"   å“åº”: {response.content}")
    
    print("\n" + "-" * 50)
    print("ğŸ’¡ æ¨èï¼šç®€å•åœºæ™¯ç”¨å­—ç¬¦ä¸²/å…ƒç»„ï¼Œå¤šæ¨¡æ€ç”¨Messageå¯¹è±¡")
    print()


# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼š6 ç§è°ƒç”¨æ–¹å¼
# ============================================================

# ç»Ÿä¸€ä½¿ç”¨å…ƒç»„æ ¼å¼ä½œä¸ºç¤ºä¾‹
messages = [
    ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œç”¨ä¸€å¥è¯ç®€æ´å›ç­”"),
    ("human", "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ"),
]

batch_messages = [
    [("system", "ç”¨ä¸€å¥è¯å›ç­”"), ("human", "ä»€ä¹ˆæ˜¯Javaï¼Ÿ")],
    [("system", "ç”¨ä¸€å¥è¯å›ç­”"), ("human", "ä»€ä¹ˆæ˜¯Goï¼Ÿ")],
    [("system", "ç”¨ä¸€å¥è¯å›ç­”"), ("human", "ä»€ä¹ˆæ˜¯Rustï¼Ÿ")],
]


def demo_invoke():
    """1. invoke() - åŒæ­¥è°ƒç”¨"""
    print("=" * 50)
    print("1. invoke() - åŒæ­¥è°ƒç”¨")
    print("=" * 50)
    response = llm.invoke(messages)
    print(f"å“åº”: {response.content}\n")


def demo_stream():
    """2. stream() - åŒæ­¥æµå¼"""
    print("=" * 50)
    print("2. stream() - åŒæ­¥æµå¼ï¼ˆè§‚å¯Ÿé€å­—è¾“å‡ºæ•ˆæœï¼‰")
    print("=" * 50)
    print("å“åº”: ", end="")
    # ç”¨æ›´é•¿çš„é—®é¢˜è®©æµå¼æ•ˆæœæ›´æ˜æ˜¾
    stream_messages = [
        ("system", "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹ä¸“å®¶"),
        ("human", "ç”¨100å­—å·¦å³ä»‹ç»Pythonçš„ä¸»è¦ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯"),
    ]
    for chunk in llm.stream(stream_messages):
        print(chunk.content, end="", flush=True)
    print("\n")


def demo_batch():
    """3. batch() - åŒæ­¥æ‰¹é‡"""
    print("=" * 50)
    print("3. batch() - åŒæ­¥æ‰¹é‡å¤„ç†")
    print("=" * 50)
    responses = llm.batch(batch_messages)
    for i, resp in enumerate(responses):
        print(f"å“åº”{i+1}: {resp.content}")
    print()


async def demo_ainvoke():
    """4. ainvoke() - å¼‚æ­¥è°ƒç”¨"""
    print("=" * 50)
    print("4. ainvoke() - å¼‚æ­¥è°ƒç”¨")
    print("=" * 50)
    response = await llm.ainvoke(messages)
    print(f"å“åº”: {response.content}\n")


async def demo_astream():
    """5. astream() - å¼‚æ­¥æµå¼"""
    print("=" * 50)
    print("5. astream() - å¼‚æ­¥æµå¼ï¼ˆè§‚å¯Ÿé€å­—è¾“å‡ºæ•ˆæœï¼‰")
    print("=" * 50)
    print("å“åº”: ", end="")
    stream_messages = [
        ("system", "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹ä¸“å®¶"),
        ("human", "ç”¨100å­—å·¦å³ä»‹ç»Javaçš„ä¸»è¦ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯"),
    ]
    async for chunk in llm.astream(stream_messages):
        print(chunk.content, end="", flush=True)
    print("\n")


async def demo_abatch():
    """6. abatch() - å¼‚æ­¥æ‰¹é‡"""
    print("=" * 50)
    print("6. abatch() - å¼‚æ­¥æ‰¹é‡å¤„ç†")
    print("=" * 50)
    responses = await llm.abatch(batch_messages)
    for i, resp in enumerate(responses):
        print(f"å“åº”{i+1}: {resp.content}")
    print()


async def demo_concurrent():
    """7. å¹¶å‘è°ƒç”¨ - ä¼ä¸šçº§æ¨è"""
    print("=" * 50)
    print("7. å¹¶å‘è°ƒç”¨ - ä¼ä¸šçº§æ¨è")
    print("=" * 50)
    tasks = [
        llm.ainvoke("1+1=?"),
        llm.ainvoke("2+2=?"),
        llm.ainvoke("3+3=?"),
    ]
    results = await asyncio.gather(*tasks)
    for i, resp in enumerate(results):
        print(f"å¹¶å‘å“åº”{i+1}: {resp.content}")
    print()


async def run_async_demos():
    await demo_ainvoke()
    await demo_astream()
    await demo_abatch()
    await demo_concurrent()


def main():
    print("\nğŸš€ LangChain åŸºç¡€æ•™ç¨‹\n")
    
    # æ˜¾ç¤ºæ”¯æŒçš„æä¾›å•†
    list_providers()
    print()
    
    # æ¶ˆæ¯æ ¼å¼æ¼”ç¤º
    demo_message_formats()
    
    # è°ƒç”¨æ–¹å¼æ¼”ç¤º
    demo_invoke()
    demo_stream()
    demo_batch()
    asyncio.run(run_async_demos())
    
    print("=" * 50)
    print("ğŸ“Œ æ€»ç»“")
    print("=" * 50)
    print("""
    æ¶ˆæ¯æ ¼å¼              é€‚ç”¨åœºæ™¯
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    å­—ç¬¦ä¸²                æœ€ç®€å•çš„å•è½®é—®ç­”
    å…ƒç»„åˆ—è¡¨              å¤šè§’è‰²å¯¹è¯ï¼ˆæ¨èï¼‰
    å­—å…¸åˆ—è¡¨              OpenAI æ ¼å¼å…¼å®¹
    Messageå¯¹è±¡           å¤šæ¨¡æ€ã€éœ€è¦å…ƒæ•°æ®
    
    è°ƒç”¨æ–¹å¼              é€‚ç”¨åœºæ™¯
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    invoke()              ç®€å•è„šæœ¬/æµ‹è¯•
    stream()              ç”¨æˆ·å®æ—¶äº¤äº’
    batch()               æ‰¹é‡æ•°æ®å¤„ç†
    ainvoke()             Web API æœåŠ¡
    astream()             å¼‚æ­¥å®æ—¶äº¤äº’
    abatch()              å¼‚æ­¥æ‰¹é‡å¤„ç†
    asyncio.gather()      é«˜å¹¶å‘åœºæ™¯
    """)


if __name__ == "__main__":
    main()
