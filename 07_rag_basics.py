"""
ç¬¬ä¸ƒè¯¾ï¼šRAG åŸºç¡€ (æ£€ç´¢å¢å¼ºç”Ÿæˆ) - è®© LLM æ‹¥æœ‰å¤–éƒ¨çŸ¥è¯†

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£ RAG çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®Œæ•´æµç¨‹
2. æŒæ¡æ–‡æ¡£åŠ è½½å™¨ï¼ˆDocument Loadersï¼‰çš„ä½¿ç”¨
3. å­¦ä¼šæ–‡æœ¬åˆ†å‰²ï¼ˆText Splittersï¼‰çš„å„ç§ç­–ç•¥
4. ç†è§£å‘é‡å­˜å‚¨ï¼ˆVector Storesï¼‰å’ŒåµŒå…¥ï¼ˆEmbeddingsï¼‰
5. æŒæ¡æ£€ç´¢å™¨ï¼ˆRetrieversï¼‰çš„å¤šç§ç±»å‹
6. æ„å»ºå®Œæ•´çš„ RAG åº”ç”¨

æ ¸å¿ƒæ¦‚å¿µï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RAG (Retrieval-Augmented Generation) æ£€ç´¢å¢å¼ºç”Ÿæˆï¼š
- LLM çš„çŸ¥è¯†æœ‰æˆªæ­¢æ—¥æœŸï¼Œä¸”æ— æ³•è®¿é—®ç§æœ‰æ•°æ®
- RAG é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“ï¼Œå°†ç›¸å…³ä¿¡æ¯æ³¨å…¥åˆ° Prompt ä¸­
- è®© LLM èƒ½å¤ŸåŸºäºæœ€æ–°/ç§æœ‰æ•°æ®ç”Ÿæˆå‡†ç¡®å›ç­”

RAG å®Œæ•´æµç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç´¢å¼•é˜¶æ®µ (Indexing) - ç¦»çº¿å¤„ç†                                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚ â”‚ æ–‡æ¡£åŠ è½½ â”‚ â†’ â”‚ æ–‡æœ¬åˆ†å‰² â”‚ â†’ â”‚ å‘é‡åµŒå…¥ â”‚ â†’ â”‚ å­˜å…¥å‘é‡åº“â”‚                  â”‚
â”‚ â”‚ Loaders  â”‚   â”‚ Splittersâ”‚   â”‚Embeddingsâ”‚   â”‚VectorStoreâ”‚                  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ£€ç´¢é˜¶æ®µ (Retrieval) - åœ¨çº¿æŸ¥è¯¢                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚ â”‚ ç”¨æˆ·é—®é¢˜ â”‚ â†’ â”‚ å‘é‡æ£€ç´¢ â”‚ â†’ â”‚ æ„å»ºPromptâ”‚ â†’ â”‚ LLMç”Ÿæˆ  â”‚                  â”‚
â”‚ â”‚  Query   â”‚   â”‚ Retrieverâ”‚   â”‚ + Context â”‚   â”‚  Answer  â”‚                  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æœ¬è¯¾æ¶µç›–çš„ RAG æ„å»ºæ–¹å¼ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–¹å¼               â”‚ é€‚ç”¨åœºæ™¯            â”‚ ç‰¹ç‚¹                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ‰‹åŠ¨ LCEL é“¾       â”‚ å­¦ä¹ ç†è§£/ç®€å•åœºæ™¯   â”‚ å®Œå…¨æ§åˆ¶ï¼Œç†è§£åŸç†              â”‚
â”‚ create_retrieval_  â”‚ æ ‡å‡† RAG åœºæ™¯       â”‚ å®˜æ–¹æ¨èï¼Œè‡ªåŠ¨å¤„ç†æ–‡æ¡£æ ¼å¼      â”‚
â”‚ chain              â”‚                     â”‚                                 â”‚
â”‚ é«˜çº§æ£€ç´¢å™¨         â”‚ å¤æ‚æ£€ç´¢éœ€æ±‚        â”‚ å¤šæŸ¥è¯¢/è‡ªæŸ¥è¯¢/ä¸Šä¸‹æ–‡å‹ç¼©        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
import os
from dotenv import load_dotenv
from llm_factory import get_llm

load_dotenv()
llm = get_llm()


# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡æ¡£åŠ è½½å™¨ (Document Loaders)
# ============================================================
"""
æ–‡æ¡£åŠ è½½å™¨è´Ÿè´£ä»å„ç§æ¥æºåŠ è½½æ•°æ®ï¼Œè½¬æ¢ä¸º LangChain çš„ Document å¯¹è±¡ã€‚

Document å¯¹è±¡ç»“æ„ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document                                                    â”‚
â”‚ â”œâ”€â”€ page_content: str  # æ–‡æ¡£çš„æ–‡æœ¬å†…å®¹                     â”‚
â”‚ â””â”€â”€ metadata: dict     # å…ƒæ•°æ®ï¼ˆæ¥æºã€é¡µç ã€ä½œè€…ç­‰ï¼‰       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¸¸ç”¨æ–‡æ¡£åŠ è½½å™¨ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åŠ è½½å™¨                 â”‚ ç”¨é€”                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TextLoader             â”‚ çº¯æ–‡æœ¬æ–‡ä»¶ (.txt)                  â”‚
â”‚ PyPDFLoader            â”‚ PDF æ–‡ä»¶                           â”‚
â”‚ CSVLoader              â”‚ CSV æ–‡ä»¶                           â”‚
â”‚ JSONLoader             â”‚ JSON æ–‡ä»¶                          â”‚
â”‚ UnstructuredLoader     â”‚ å¤šç§æ ¼å¼ï¼ˆPDF/Word/HTMLç­‰ï¼‰        â”‚
â”‚ WebBaseLoader          â”‚ ç½‘é¡µå†…å®¹                           â”‚
â”‚ DirectoryLoader        â”‚ æ•´ä¸ªç›®å½•çš„æ–‡ä»¶                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

def demo_document_loaders():
    """1.1 æ–‡æ¡£åŠ è½½å™¨æ¼”ç¤º"""
    print("=" * 60)
    print("1.1 æ–‡æ¡£åŠ è½½å™¨ (Document Loaders)")
    print("=" * 60)
    
    from langchain_core.documents import Document
    
    # æ–¹å¼1ï¼šæ‰‹åŠ¨åˆ›å»º Documentï¼ˆç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•ï¼‰
    print("\n--- æ‰‹åŠ¨åˆ›å»º Document ---")
    docs = [
        Document(
            page_content="LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„æ¡†æ¶ã€‚å®ƒæä¾›äº†æ¨¡å—åŒ–ç»„ä»¶ã€‚",
            metadata={"source": "langchain_intro.txt", "page": 1}
        ),
        Document(
            page_content="RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆæ¥æé«˜å›ç­”è´¨é‡ã€‚",
            metadata={"source": "rag_intro.txt", "page": 1}
        ),
        Document(
            page_content="å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡åµŒå…¥ï¼Œæ”¯æŒè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ã€‚",
            metadata={"source": "vector_db.txt", "page": 1}
        ),
    ]
    
    for doc in docs:
        print(f"å†…å®¹: {doc.page_content[:30]}...")
        print(f"å…ƒæ•°æ®: {doc.metadata}")
        print()
    
    return docs


def demo_text_loader():
    """1.2 TextLoader ç¤ºä¾‹"""
    print("=" * 60)
    print("1.2 TextLoader - åŠ è½½æ–‡æœ¬æ–‡ä»¶")
    print("=" * 60)
    
    # å…ˆåˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶
    sample_text = """LangChain æ¡†æ¶ä»‹ç»

LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œç”¨äºå¼€å‘ç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„åº”ç”¨ç¨‹åºã€‚

ä¸»è¦ç‰¹ç‚¹ï¼š
1. æ¨¡å—åŒ–è®¾è®¡ï¼šæä¾›å¯ç»„åˆçš„ç»„ä»¶
2. é“¾å¼è°ƒç”¨ï¼šæ”¯æŒå¤æ‚çš„å·¥ä½œæµ
3. ä¸°å¯Œçš„é›†æˆï¼šæ”¯æŒå¤šç§ LLM å’Œå·¥å…·

RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ LangChain çš„æ ¸å¿ƒåº”ç”¨åœºæ™¯ä¹‹ä¸€ã€‚
"""
    
    # å†™å…¥ä¸´æ—¶æ–‡ä»¶
    with open("_temp_sample.txt", "w", encoding="utf-8") as f:
        f.write(sample_text)
    
    try:
        from langchain_community.document_loaders import TextLoader
        
        loader = TextLoader("_temp_sample.txt", encoding="utf-8")
        documents = loader.load()
        
        print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
        print(f"å†…å®¹é¢„è§ˆ: {documents[0].page_content[:100]}...")
        print(f"å…ƒæ•°æ®: {documents[0].metadata}")
        
    except ImportError:
        print("éœ€è¦å®‰è£…: pip install langchain-community")
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists("_temp_sample.txt"):
            os.remove("_temp_sample.txt")
    print()


def demo_web_loader():
    """1.3 WebBaseLoader ç¤ºä¾‹"""
    print("=" * 60)
    print("1.3 WebBaseLoader - åŠ è½½ç½‘é¡µå†…å®¹")
    print("=" * 60)
    
    try:
        from langchain_community.document_loaders import WebBaseLoader
        
        # åŠ è½½ç½‘é¡µï¼ˆéœ€è¦ç½‘ç»œè¿æ¥ï¼‰
        # loader = WebBaseLoader("https://python.langchain.com/docs/introduction/")
        # documents = loader.load()
        
        print("WebBaseLoader ç”¨æ³•ç¤ºä¾‹ï¼š")
        print("""
from langchain_community.document_loaders import WebBaseLoader

# åŠ è½½å•ä¸ªç½‘é¡µ
loader = WebBaseLoader("https://example.com/page")
docs = loader.load()

# åŠ è½½å¤šä¸ªç½‘é¡µ
loader = WebBaseLoader([
    "https://example.com/page1",
    "https://example.com/page2"
])
docs = loader.load()

# ğŸ’¡ æç¤ºï¼šéœ€è¦å®‰è£… beautifulsoup4
# pip install beautifulsoup4
""")
    except ImportError:
        print("éœ€è¦å®‰è£…: pip install langchain-community beautifulsoup4")
    print()


def demo_directory_loader():
    """1.4 DirectoryLoader ç¤ºä¾‹"""
    print("=" * 60)
    print("1.4 DirectoryLoader - åŠ è½½æ•´ä¸ªç›®å½•")
    print("=" * 60)
    
    print("""
DirectoryLoader ç”¨æ³•ç¤ºä¾‹ï¼š

from langchain_community.document_loaders import DirectoryLoader, TextLoader

# åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶
loader = DirectoryLoader(
    path="./documents",
    glob="**/*.txt",           # åŒ¹é…æ¨¡å¼
    loader_cls=TextLoader,     # ä½¿ç”¨çš„åŠ è½½å™¨ç±»
    show_progress=True,        # æ˜¾ç¤ºè¿›åº¦æ¡
    use_multithreading=True,   # å¤šçº¿ç¨‹åŠ è½½
)
docs = loader.load()

# åŠ è½½ PDF æ–‡ä»¶
from langchain_community.document_loaders import PyPDFLoader
loader = DirectoryLoader(
    path="./pdfs",
    glob="**/*.pdf",
    loader_cls=PyPDFLoader,
)

# ğŸ’¡ æç¤ºï¼š
# - glob æ”¯æŒé€’å½’åŒ¹é… (**)
# - å¯ä»¥æŒ‡å®šä¸åŒçš„ loader_cls å¤„ç†ä¸åŒæ ¼å¼
""")
    print()


# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ–‡æœ¬åˆ†å‰²å™¨ (Text Splitters)
# ============================================================
"""
ä¸ºä»€ä¹ˆéœ€è¦æ–‡æœ¬åˆ†å‰²ï¼Ÿ
1. LLM æœ‰ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼ˆå¦‚ 4Kã€8Kã€128K tokensï¼‰
2. å¤ªé•¿çš„æ–‡æœ¬ä¼šå¯¼è‡´æ£€ç´¢ä¸ç²¾ç¡®
3. é€‚å½“å¤§å°çš„å—èƒ½æé«˜æ£€ç´¢è´¨é‡

åˆ†å‰²ç­–ç•¥å¯¹æ¯”ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åˆ†å‰²å™¨                     â”‚ ç‰¹ç‚¹                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CharacterTextSplitter      â”‚ æŒ‰å­—ç¬¦æ•°åˆ†å‰²ï¼Œç®€å•ä½†å¯èƒ½åˆ‡æ–­å¥å­           â”‚
â”‚ RecursiveCharacterText     â”‚ é€’å½’åˆ†å‰²ï¼Œä¼˜å…ˆä¿æŒæ®µè½/å¥å­å®Œæ•´ï¼ˆæ¨èï¼‰    â”‚
â”‚ Splitter                   â”‚                                            â”‚
â”‚ TokenTextSplitter          â”‚ æŒ‰ token æ•°åˆ†å‰²ï¼Œæ›´ç²¾ç¡®æ§åˆ¶                â”‚
â”‚ SentenceTransformers       â”‚ æŒ‰è¯­ä¹‰åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§                 â”‚
â”‚ TextSplitter               â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®å‚æ•°ï¼š
- chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å¤§å°
- chunk_overlap: å—ä¹‹é—´çš„é‡å å¤§å°ï¼ˆä¿æŒä¸Šä¸‹æ–‡è¿è´¯ï¼‰
- separators: åˆ†å‰²ç¬¦ä¼˜å…ˆçº§åˆ—è¡¨
"""

def demo_text_splitters():
    """2.1 æ–‡æœ¬åˆ†å‰²å™¨æ¼”ç¤º"""
    print("=" * 60)
    print("2.1 æ–‡æœ¬åˆ†å‰²å™¨ (Text Splitters)")
    print("=" * 60)
    
    from langchain_text_splitters import (
        CharacterTextSplitter,
        RecursiveCharacterTextSplitter,
    )
    
    # ç¤ºä¾‹é•¿æ–‡æœ¬
    long_text = """
äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§°AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯AIçš„å¦ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯NLPé¢†åŸŸçš„æœ€æ–°çªç ´ï¼Œå¦‚GPTã€Claudeã€Qwenç­‰ã€‚

LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†ä¸°å¯Œçš„ç»„ä»¶æ¥æ„å»ºå¤æ‚çš„ AI åº”ç”¨ã€‚RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯å…¶æ ¸å¿ƒåº”ç”¨åœºæ™¯ä¹‹ä¸€ã€‚
"""
    
    print("\n--- CharacterTextSplitterï¼ˆç®€å•å­—ç¬¦åˆ†å‰²ï¼‰---")
    char_splitter = CharacterTextSplitter(
        separator="\n\n",      # åˆ†å‰²ç¬¦
        chunk_size=100,        # å—å¤§å°
        chunk_overlap=20,      # é‡å å¤§å°
    )
    char_chunks = char_splitter.split_text(long_text)
    print(f"åˆ†å‰²æˆ {len(char_chunks)} ä¸ªå—")
    for i, chunk in enumerate(char_chunks[:2]):
        print(f"å— {i+1}: {chunk[:50]}...")

    print("\n--- RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰---")
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=100,
        chunk_overlap=20,
        # åˆ†å‰²ç¬¦ä¼˜å…ˆçº§ï¼šå…ˆå°è¯•æ®µè½ï¼Œå†å¥å­ï¼Œå†é€—å·ï¼Œæœ€åå­—ç¬¦
        separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
    )
    recursive_chunks = recursive_splitter.split_text(long_text)
    print(f"åˆ†å‰²æˆ {len(recursive_chunks)} ä¸ªå—")
    for i, chunk in enumerate(recursive_chunks[:3]):
        print(f"å— {i+1} ({len(chunk)} å­—ç¬¦): {chunk.strip()[:50]}...")
    
    print("\nğŸ’¡ RecursiveCharacterTextSplitter æ˜¯æœ€å¸¸ç”¨çš„åˆ†å‰²å™¨")
    print("   å®ƒä¼šé€’å½’å°è¯•ä¸åŒçš„åˆ†å‰²ç¬¦ï¼Œå°½é‡ä¿æŒæ–‡æœ¬çš„è¯­ä¹‰å®Œæ•´æ€§")
    print()


def demo_split_documents():
    """2.2 åˆ†å‰² Document å¯¹è±¡"""
    print("=" * 60)
    print("2.2 åˆ†å‰² Document å¯¹è±¡ï¼ˆä¿ç•™å…ƒæ•°æ®ï¼‰")
    print("=" * 60)
    
    from langchain_core.documents import Document
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    # åˆ›å»ºæ–‡æ¡£
    doc = Document(
        page_content="""
LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œç”¨äºå¼€å‘ç”±å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºã€‚

å®ƒçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
1. æ¨¡å—åŒ–è®¾è®¡ï¼šæä¾›å¯ç»„åˆçš„ç»„ä»¶ï¼Œå¦‚ Promptsã€LLMsã€Chains ç­‰
2. é“¾å¼è°ƒç”¨ï¼šæ”¯æŒå¤æ‚çš„å·¥ä½œæµç¼–æ’
3. ä¸°å¯Œçš„é›†æˆï¼šæ”¯æŒå¤šç§ LLM æä¾›å•†å’Œå¤–éƒ¨å·¥å…·

RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ LangChain çš„æ ¸å¿ƒåº”ç”¨åœºæ™¯ä¹‹ä¸€ï¼Œå®ƒé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†æ¥å¢å¼º LLM çš„å›ç­”èƒ½åŠ›ã€‚
""",
        metadata={"source": "langchain_guide.txt", "author": "AIæ•™ç¨‹"}
    )
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=100,
        chunk_overlap=20,
    )
    
    # split_documents ä¼šä¿ç•™å¹¶ä¼ é€’å…ƒæ•°æ®
    chunks = splitter.split_documents([doc])
    
    print(f"åŸæ–‡æ¡£åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")
    for i, chunk in enumerate(chunks):
        print(f"\nå— {i+1}:")
        print(f"  å†…å®¹: {chunk.page_content[:40]}...")
        print(f"  å…ƒæ•°æ®: {chunk.metadata}")
    print()


# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‘é‡åµŒå…¥ (Embeddings)
# ============================================================
"""
åµŒå…¥ï¼ˆEmbeddingï¼‰æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡çš„è¿‡ç¨‹ã€‚
ç›¸ä¼¼çš„æ–‡æœ¬ä¼šæœ‰ç›¸ä¼¼çš„å‘é‡è¡¨ç¤ºï¼Œè¿™æ˜¯è¯­ä¹‰æœç´¢çš„åŸºç¡€ã€‚

å¸¸ç”¨åµŒå…¥æ¨¡å‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¨¡å‹                       â”‚ ç‰¹ç‚¹                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OpenAI text-embedding-3    â”‚ é«˜è´¨é‡ï¼Œéœ€è¦ API Keyï¼Œæ”¶è´¹                 â”‚
â”‚ HuggingFace æ¨¡å‹           â”‚ å¼€æºå…è´¹ï¼Œå¯æœ¬åœ°è¿è¡Œ                       â”‚
â”‚ é˜¿é‡Œäº‘ DashScope           â”‚ å›½å†…è®¿é—®å¿«ï¼Œæ”¯æŒä¸­æ–‡                       â”‚
â”‚ æ™ºè°± embedding             â”‚ å›½å†…æœåŠ¡ï¼Œä¸­æ–‡æ•ˆæœå¥½                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ æœ¬æ•™ç¨‹ä½¿ç”¨ DashScope çš„åµŒå…¥æ¨¡å‹ï¼ˆä¸ qwen åŒä¸€å¹³å°ï¼‰
"""

# å¯¼å…¥åµŒå…¥æ¨¡å‹å·¥å‚
from embedding_factory import get_embeddings


def demo_embeddings():
    """3.1 åµŒå…¥æ¨¡å‹æ¼”ç¤º"""
    print("=" * 60)
    print("3.1 åµŒå…¥æ¨¡å‹ (Embeddings)")
    print("=" * 60)
    
    try:
        embeddings = get_embeddings()
        
        # åµŒå…¥å•ä¸ªæ–‡æœ¬
        text = "LangChain æ˜¯ä¸€ä¸ª LLM åº”ç”¨å¼€å‘æ¡†æ¶"
        vector = embeddings.embed_query(text)
        
        print(f"æ–‡æœ¬: {text}")
        print(f"å‘é‡ç»´åº¦: {len(vector)}")
        print(f"å‘é‡å‰5ä¸ªå€¼: {vector[:5]}")
        
        # åµŒå…¥å¤šä¸ªæ–‡æœ¬
        texts = [
            "LangChain æ˜¯ä¸€ä¸ªæ¡†æ¶",
            "RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ",
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
        ]
        vectors = embeddings.embed_documents(texts)
        
        print(f"\næ‰¹é‡åµŒå…¥ {len(texts)} ä¸ªæ–‡æœ¬")
        print(f"æ¯ä¸ªå‘é‡ç»´åº¦: {len(vectors[0])}")
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        import numpy as np
        def cosine_similarity(v1, v2):
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        
        print("\n--- è¯­ä¹‰ç›¸ä¼¼åº¦æ¼”ç¤º ---")
        query = "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ"
        query_vec = embeddings.embed_query(query)
        
        for i, (text, vec) in enumerate(zip(texts, vectors)):
            sim = cosine_similarity(query_vec, vec)
            print(f"'{query}' vs '{text}': {sim:.4f}")
        
    except Exception as e:
        print(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿è®¾ç½®äº† DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
    print()


# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå‘é‡å­˜å‚¨ (Vector Stores)
# ============================================================
"""
å‘é‡å­˜å‚¨ç”¨äºå­˜å‚¨åµŒå…¥å‘é‡å¹¶æ”¯æŒç›¸ä¼¼åº¦æœç´¢ã€‚

å¸¸ç”¨å‘é‡å­˜å‚¨ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å‘é‡å­˜å‚¨                   â”‚ ç‰¹ç‚¹                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chroma                     â”‚ è½»é‡çº§ï¼Œæ”¯æŒæŒä¹…åŒ–ï¼Œé€‚åˆå¼€å‘å’Œå°è§„æ¨¡       â”‚
â”‚ FAISS                      â”‚ Facebook å¼€æºï¼Œé«˜æ€§èƒ½ï¼Œé€‚åˆå¤§è§„æ¨¡          â”‚
â”‚ Pinecone                   â”‚ äº‘æœåŠ¡ï¼Œå…¨æ‰˜ç®¡ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ               â”‚
â”‚ Milvus                     â”‚ å¼€æºåˆ†å¸ƒå¼ï¼Œé€‚åˆä¼ä¸šçº§åº”ç”¨                 â”‚
â”‚ Qdrant                     â”‚ Rust å®ç°ï¼Œé«˜æ€§èƒ½ï¼Œæ”¯æŒè¿‡æ»¤                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ æœ¬æ•™ç¨‹ä½¿ç”¨ Chromaï¼ˆè½»é‡çº§ï¼Œæ— éœ€é¢å¤–æœåŠ¡ï¼‰
   å®‰è£…: pip install langchain-chroma chromadb
"""

def demo_vector_store():
    """4.1 å‘é‡å­˜å‚¨åŸºç¡€"""
    print("=" * 60)
    print("4.1 å‘é‡å­˜å‚¨ (Vector Store) - Chroma")
    print("=" * 60)
    
    try:
        from langchain_chroma import Chroma
        from langchain_core.documents import Document
        
        embeddings = get_embeddings()
        
        # å‡†å¤‡æ–‡æ¡£
        docs = [
            Document(page_content="LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„æ¡†æ¶", 
                     metadata={"source": "intro", "topic": "langchain"}),
            Document(page_content="RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆæ£€ç´¢å’Œç”ŸæˆæŠ€æœ¯", 
                     metadata={"source": "rag", "topic": "rag"}),
            Document(page_content="å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡åµŒå…¥", 
                     metadata={"source": "vector", "topic": "database"}),
            Document(page_content="Prompt å·¥ç¨‹æ˜¯è®¾è®¡å’Œä¼˜åŒ–æç¤ºè¯çš„è¿‡ç¨‹", 
                     metadata={"source": "prompt", "topic": "prompt"}),
            Document(page_content="Agent æ˜¯èƒ½å¤Ÿè‡ªä¸»å†³ç­–å’Œä½¿ç”¨å·¥å…·çš„ AI ç³»ç»Ÿ", 
                     metadata={"source": "agent", "topic": "agent"}),
        ]
        
        # åˆ›å»ºå‘é‡å­˜å‚¨ï¼ˆå†…å­˜æ¨¡å¼ï¼‰
        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=embeddings,
            collection_name="demo_collection"
        )
        
        print(f"å·²å­˜å‚¨ {len(docs)} ä¸ªæ–‡æ¡£åˆ°å‘é‡åº“")
        
        # ç›¸ä¼¼åº¦æœç´¢
        print("\n--- ç›¸ä¼¼åº¦æœç´¢ ---")
        query = "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"
        results = vectorstore.similarity_search(query, k=2)
        
        print(f"æŸ¥è¯¢: {query}")
        for i, doc in enumerate(results):
            print(f"ç»“æœ {i+1}: {doc.page_content}")
            print(f"        å…ƒæ•°æ®: {doc.metadata}")
        
        # å¸¦åˆ†æ•°çš„æœç´¢
        print("\n--- å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æœç´¢ ---")
        results_with_scores = vectorstore.similarity_search_with_score(query, k=3)
        
        for doc, score in results_with_scores:
            print(f"åˆ†æ•°: {score:.4f} | {doc.page_content[:30]}...")
        
        return vectorstore
        
    except ImportError as e:
        print(f"éœ€è¦å®‰è£…ä¾èµ–: {e}")
        print("pip install langchain-chroma chromadb")
        return None
    print()


def demo_vector_store_persistence():
    """4.2 å‘é‡å­˜å‚¨æŒä¹…åŒ–"""
    print("=" * 60)
    print("4.2 å‘é‡å­˜å‚¨æŒä¹…åŒ–")
    print("=" * 60)
    
    print("""
Chroma æŒä¹…åŒ–ç¤ºä¾‹ï¼š

from langchain_chroma import Chroma

# åˆ›å»ºæŒä¹…åŒ–å‘é‡å­˜å‚¨
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embeddings,
    persist_directory="./chroma_db",  # æŒä¹…åŒ–ç›®å½•
    collection_name="my_collection"
)

# åç»­åŠ è½½å·²æœ‰çš„å‘é‡å­˜å‚¨
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings,
    collection_name="my_collection"
)

# æ·»åŠ æ–°æ–‡æ¡£
vectorstore.add_documents(new_docs)

# åˆ é™¤æ–‡æ¡£ï¼ˆé€šè¿‡ IDï¼‰
vectorstore.delete(ids=["doc_id_1", "doc_id_2"])

ğŸ’¡ æç¤ºï¼š
- persist_directory æŒ‡å®šå­˜å‚¨ç›®å½•
- é‡å¯ç¨‹åºåå¯ä»¥ç›´æ¥åŠ è½½ï¼Œæ— éœ€é‡æ–°åµŒå…¥
- é€‚åˆéœ€è¦æŒä¹…åŒ–çš„ç”Ÿäº§åœºæ™¯
""")
    print()


def demo_faiss_vector_store():
    """4.3 FAISS å‘é‡å­˜å‚¨"""
    print("=" * 60)
    print("4.3 FAISS å‘é‡å­˜å‚¨ï¼ˆé«˜æ€§èƒ½ï¼‰")
    print("=" * 60)
    
    print("""
FAISS æ˜¯ Facebook å¼€æºçš„é«˜æ€§èƒ½å‘é‡æœç´¢åº“ï¼š

# å®‰è£…
pip install faiss-cpu  # CPU ç‰ˆæœ¬
# pip install faiss-gpu  # GPU ç‰ˆæœ¬ï¼ˆéœ€è¦ CUDAï¼‰

from langchain_community.vectorstores import FAISS

# ä»æ–‡æ¡£åˆ›å»º
vectorstore = FAISS.from_documents(docs, embeddings)

# ä»æ–‡æœ¬åˆ›å»º
vectorstore = FAISS.from_texts(
    texts=["æ–‡æœ¬1", "æ–‡æœ¬2"],
    embedding=embeddings,
    metadatas=[{"source": "a"}, {"source": "b"}]
)

# ä¿å­˜åˆ°æœ¬åœ°
vectorstore.save_local("./faiss_index")

# åŠ è½½
vectorstore = FAISS.load_local(
    "./faiss_index", 
    embeddings,
    allow_dangerous_deserialization=True  # ä¿¡ä»»æœ¬åœ°æ–‡ä»¶
)

# åˆå¹¶ä¸¤ä¸ªå‘é‡å­˜å‚¨
vectorstore1.merge_from(vectorstore2)

ğŸ’¡ FAISS vs Chromaï¼š
- FAISSï¼šæ›´é«˜æ€§èƒ½ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®
- Chromaï¼šæ›´æ˜“ç”¨ï¼Œå†…ç½®æŒä¹…åŒ–ï¼Œé€‚åˆå¼€å‘
""")
    print()


# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šæ£€ç´¢å™¨ (Retrievers)
# ============================================================
"""
æ£€ç´¢å™¨æ˜¯ RAG çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£æ ¹æ®æŸ¥è¯¢æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚

æ£€ç´¢å™¨ç±»å‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ£€ç´¢å™¨                     â”‚ ç‰¹ç‚¹                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VectorStoreRetriever       â”‚ åŸºç¡€å‘é‡æ£€ç´¢ï¼Œæœ€å¸¸ç”¨                       â”‚
â”‚ MultiQueryRetriever        â”‚ ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“ï¼Œæé«˜å¬å›ç‡               â”‚
â”‚ SelfQueryRetriever         â”‚ è‡ªåŠ¨ä»é—®é¢˜ä¸­æå–è¿‡æ»¤æ¡ä»¶                   â”‚
â”‚ ContextualCompression      â”‚ å‹ç¼©æ£€ç´¢ç»“æœï¼Œåªä¿ç•™ç›¸å…³éƒ¨åˆ†               â”‚
â”‚ Retriever                  â”‚                                            â”‚
â”‚ ParentDocumentRetriever    â”‚ æ£€ç´¢å°å—ï¼Œè¿”å›çˆ¶æ–‡æ¡£                       â”‚
â”‚ EnsembleRetriever          â”‚ ç»„åˆå¤šä¸ªæ£€ç´¢å™¨                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

def demo_basic_retriever():
    """5.1 åŸºç¡€æ£€ç´¢å™¨"""
    print("=" * 60)
    print("5.1 åŸºç¡€æ£€ç´¢å™¨ (VectorStoreRetriever)")
    print("=" * 60)
    
    try:
        from langchain_chroma import Chroma
        from langchain_core.documents import Document
        
        embeddings = get_embeddings()
        
        docs = [
            Document(page_content="Python æ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œç®€å•æ˜“å­¦", 
                     metadata={"topic": "python"}),
            Document(page_content="JavaScript æ˜¯ç½‘é¡µå¼€å‘çš„æ ¸å¿ƒè¯­è¨€", 
                     metadata={"topic": "javascript"}),
            Document(page_content="LangChain æ˜¯ç”¨ Python å¼€å‘çš„ LLM æ¡†æ¶", 
                     metadata={"topic": "langchain"}),
            Document(page_content="React æ˜¯ä¸€ä¸ª JavaScript å‰ç«¯æ¡†æ¶", 
                     metadata={"topic": "react"}),
        ]
        
        vectorstore = Chroma.from_documents(docs, embeddings)
        
        # æ–¹å¼1ï¼šas_retriever() è½¬æ¢ä¸ºæ£€ç´¢å™¨
        retriever = vectorstore.as_retriever(
            search_type="similarity",  # æœç´¢ç±»å‹
            search_kwargs={"k": 2}     # è¿”å› top-k ç»“æœ
        )
        
        print("--- åŸºç¡€ç›¸ä¼¼åº¦æ£€ç´¢ ---")
        results = retriever.invoke("Python ç¼–ç¨‹")
        for doc in results:
            print(f"  - {doc.page_content}")
        
        # æ–¹å¼2ï¼šMMR (æœ€å¤§è¾¹é™…ç›¸å…³æ€§) - å¢åŠ ç»“æœå¤šæ ·æ€§
        print("\n--- MMR æ£€ç´¢ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰---")
        retriever_mmr = vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 2,
                "fetch_k": 4,      # å…ˆè·å–æ›´å¤šå€™é€‰
                "lambda_mult": 0.5 # å¤šæ ·æ€§å‚æ•° (0=æœ€å¤§å¤šæ ·æ€§, 1=æœ€å¤§ç›¸å…³æ€§)
            }
        )
        results = retriever_mmr.invoke("ç¼–ç¨‹è¯­è¨€")
        for doc in results:
            print(f"  - {doc.page_content}")
        
        # æ–¹å¼3ï¼šå¸¦è¿‡æ»¤æ¡ä»¶çš„æ£€ç´¢
        print("\n--- å¸¦å…ƒæ•°æ®è¿‡æ»¤çš„æ£€ç´¢ ---")
        retriever_filtered = vectorstore.as_retriever(
            search_kwargs={
                "k": 2,
                "filter": {"topic": "python"}  # åªæ£€ç´¢ topic=python çš„æ–‡æ¡£
            }
        )
        results = retriever_filtered.invoke("æ¡†æ¶")
        for doc in results:
            print(f"  - {doc.page_content} (topic: {doc.metadata['topic']})")
            
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    print()


def demo_multi_query_retriever():
    """5.2 å¤šæŸ¥è¯¢æ£€ç´¢å™¨"""
    print("=" * 60)
    print("5.2 MultiQueryRetrieverï¼ˆæé«˜å¬å›ç‡ï¼‰")
    print("=" * 60)
    
    print("""
MultiQueryRetriever å·¥ä½œåŸç†ï¼š
1. ä½¿ç”¨ LLM å°†ç”¨æˆ·é—®é¢˜æ”¹å†™æˆå¤šä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢
2. å¯¹æ¯ä¸ªæŸ¥è¯¢åˆ†åˆ«æ£€ç´¢
3. åˆå¹¶å»é‡æ‰€æœ‰ç»“æœ

é€‚ç”¨åœºæ™¯ï¼š
- ç”¨æˆ·é—®é¢˜è¡¨è¿°ä¸æ¸…æ™°
- éœ€è¦ä»å¤šä¸ªè§’åº¦æ£€ç´¢ä¿¡æ¯
- æé«˜å¬å›ç‡

ç¤ºä¾‹ä»£ç ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain.retrievers.multi_query import MultiQueryRetriever

# åˆ›å»ºå¤šæŸ¥è¯¢æ£€ç´¢å™¨
multi_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm
)

# ä½¿ç”¨
results = multi_retriever.invoke("LangChain æœ‰ä»€ä¹ˆç”¨ï¼Ÿ")

# æŸ¥çœ‹ç”Ÿæˆçš„æŸ¥è¯¢å˜ä½“ï¼ˆè°ƒè¯•ç”¨ï¼‰
import logging
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.DEBUG)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ’¡ ä¼˜ç‚¹ï¼šæé«˜å¬å›ç‡ï¼Œè¦†ç›–æ›´å¤šç›¸å…³æ–‡æ¡£
ğŸ’¡ ç¼ºç‚¹ï¼šéœ€è¦é¢å¤–çš„ LLM è°ƒç”¨ï¼Œå¢åŠ å»¶è¿Ÿå’Œæˆæœ¬
""")
    print()


def demo_contextual_compression():
    """5.3 ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨"""
    print("=" * 60)
    print("5.3 ContextualCompressionRetrieverï¼ˆç²¾å‡†æå–ï¼‰")
    print("=" * 60)
    
    print("""
ContextualCompressionRetriever å·¥ä½œåŸç†ï¼š
1. å…ˆç”¨åŸºç¡€æ£€ç´¢å™¨è·å–æ–‡æ¡£
2. ä½¿ç”¨å‹ç¼©å™¨ï¼ˆLLM æˆ–å…¶ä»–ï¼‰æå–ä¸é—®é¢˜ç›¸å…³çš„éƒ¨åˆ†
3. è¿”å›å‹ç¼©åçš„ç»“æœ

é€‚ç”¨åœºæ™¯ï¼š
- æ£€ç´¢åˆ°çš„æ–‡æ¡£å¤ªé•¿
- åªéœ€è¦æ–‡æ¡£ä¸­çš„ç‰¹å®šéƒ¨åˆ†
- æé«˜ç­”æ¡ˆç²¾ç¡®åº¦

ç¤ºä¾‹ä»£ç ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# åˆ›å»ºå‹ç¼©å™¨ï¼ˆä½¿ç”¨ LLM æå–ç›¸å…³å†…å®¹ï¼‰
compressor = LLMChainExtractor.from_llm(llm)

# åˆ›å»ºå‹ç¼©æ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)

# ä½¿ç”¨
results = compression_retriever.invoke("ä»€ä¹ˆæ˜¯ RAGï¼Ÿ")
# è¿”å›çš„æ–‡æ¡£åªåŒ…å«ä¸é—®é¢˜ç›¸å…³çš„éƒ¨åˆ†
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

å…¶ä»–å‹ç¼©å™¨é€‰é¡¹ï¼š
- LLMChainFilter: ä½¿ç”¨ LLM è¿‡æ»¤ä¸ç›¸å…³æ–‡æ¡£
- EmbeddingsFilter: ä½¿ç”¨åµŒå…¥ç›¸ä¼¼åº¦è¿‡æ»¤
- DocumentCompressorPipeline: ç»„åˆå¤šä¸ªå‹ç¼©å™¨

ğŸ’¡ ä¼˜ç‚¹ï¼šæé«˜ç²¾ç¡®åº¦ï¼Œå‡å°‘æ— å…³ä¿¡æ¯
ğŸ’¡ ç¼ºç‚¹ï¼šéœ€è¦é¢å¤–çš„ LLM è°ƒç”¨
""")
    print()


def demo_self_query_retriever():
    """5.4 è‡ªæŸ¥è¯¢æ£€ç´¢å™¨"""
    print("=" * 60)
    print("5.4 SelfQueryRetrieverï¼ˆè‡ªåŠ¨æå–è¿‡æ»¤æ¡ä»¶ï¼‰")
    print("=" * 60)
    
    print("""
SelfQueryRetriever å·¥ä½œåŸç†ï¼š
1. ä½¿ç”¨ LLM åˆ†æç”¨æˆ·é—®é¢˜
2. è‡ªåŠ¨æå–è¯­ä¹‰æŸ¥è¯¢å’Œå…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
3. ç»“åˆè¯­ä¹‰æœç´¢å’Œå…ƒæ•°æ®è¿‡æ»¤

é€‚ç”¨åœºæ™¯ï¼š
- æ–‡æ¡£æœ‰ä¸°å¯Œçš„å…ƒæ•°æ®ï¼ˆæ—¥æœŸã€ç±»åˆ«ã€ä½œè€…ç­‰ï¼‰
- ç”¨æˆ·é—®é¢˜åŒ…å«è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚"2024å¹´çš„æ–‡ç« "ï¼‰

ç¤ºä¾‹ä»£ç ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.schema import AttributeInfo

# å®šä¹‰å…ƒæ•°æ®å­—æ®µ
metadata_field_info = [
    AttributeInfo(
        name="year",
        description="æ–‡æ¡£å‘å¸ƒå¹´ä»½",
        type="integer",
    ),
    AttributeInfo(
        name="category",
        description="æ–‡æ¡£ç±»åˆ«ï¼Œå¦‚ 'tutorial', 'news', 'api'",
        type="string",
    ),
]

# åˆ›å»ºè‡ªæŸ¥è¯¢æ£€ç´¢å™¨
self_query_retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vectorstore,
    document_contents="æŠ€æœ¯æ–‡æ¡£",
    metadata_field_info=metadata_field_info,
)

# ä½¿ç”¨ - LLM ä¼šè‡ªåŠ¨æå–è¿‡æ»¤æ¡ä»¶
results = self_query_retriever.invoke("2024å¹´çš„æ•™ç¨‹æ–‡æ¡£")
# è‡ªåŠ¨è½¬æ¢ä¸º: è¯­ä¹‰æœç´¢"æ•™ç¨‹" + è¿‡æ»¤ year=2024, category="tutorial"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ’¡ ä¼˜ç‚¹ï¼šè‡ªåŠ¨ç†è§£ç”¨æˆ·æ„å›¾ï¼Œæ™ºèƒ½è¿‡æ»¤
ğŸ’¡ ç¼ºç‚¹ï¼šéœ€è¦é¢„å®šä¹‰å…ƒæ•°æ®ç»“æ„
""")
    print()


# ============================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šæ„å»ºå®Œæ•´çš„ RAG é“¾
# ============================================================
"""
RAG é“¾çš„ä¸¤ç§æ„å»ºæ–¹å¼ï¼š
1. æ‰‹åŠ¨ LCEL é“¾ï¼šå®Œå…¨æ§åˆ¶ï¼Œé€‚åˆå­¦ä¹ å’Œè‡ªå®šä¹‰
2. create_retrieval_chainï¼šå®˜æ–¹æ¨èï¼Œè‡ªåŠ¨å¤„ç†æ–‡æ¡£æ ¼å¼
"""

def demo_rag_chain_manual():
    """6.1 æ‰‹åŠ¨æ„å»º RAG é“¾ï¼ˆLCEL æ–¹å¼ï¼‰"""
    print("=" * 60)
    print("6.1 æ‰‹åŠ¨æ„å»º RAG é“¾ï¼ˆLCEL æ–¹å¼ï¼‰")
    print("=" * 60)
    
    try:
        from langchain_chroma import Chroma
        from langchain_core.documents import Document
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.runnables import RunnablePassthrough
        
        embeddings = get_embeddings()
        
        # å‡†å¤‡çŸ¥è¯†åº“
        docs = [
            Document(page_content="LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„å¼€æºæ¡†æ¶ï¼Œç”± Harrison Chase åˆ›å»ºäº 2022 å¹´ã€‚"),
            Document(page_content="LangChain çš„æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬ï¼šPromptsã€LLMsã€Chainsã€Agentsã€Memory å’Œ Retrievalã€‚"),
            Document(page_content="RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†æ¥å¢å¼º LLM çš„å›ç­”èƒ½åŠ›ï¼Œå‡å°‘å¹»è§‰ã€‚"),
            Document(page_content="LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ LangChain çš„å£°æ˜å¼ç¼–æ’è¯­æ³•ï¼Œä½¿ç”¨ | ç®¡é“ç¬¦ç»„åˆç»„ä»¶ã€‚"),
            Document(page_content="LangGraph æ˜¯ LangChain å›¢é˜Ÿå¼€å‘çš„å›¾çŠ¶æ€æœºæ¡†æ¶ï¼Œç”¨äºæ„å»ºå¤æ‚çš„ Agent å·¥ä½œæµã€‚"),
        ]
        
        vectorstore = Chroma.from_documents(docs, embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
        
        # RAG æç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š""")
        
        # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        # æ„å»º RAG é“¾
        rag_chain = (
            {
                "context": retriever | format_docs,  # æ£€ç´¢ -> æ ¼å¼åŒ–
                "question": RunnablePassthrough()     # ç›´æ¥ä¼ é€’é—®é¢˜
            }
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # æµ‹è¯•
        questions = [
            "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ",
            "RAG æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ",
            "LCEL æ˜¯ä»€ä¹ˆï¼Ÿ",
            "é‡å­è®¡ç®—æ˜¯ä»€ä¹ˆï¼Ÿ"  # çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„é—®é¢˜
        ]
        
        for q in questions:
            print(f"\né—®é¢˜: {q}")
            answer = rag_chain.invoke(q)
            print(f"å›ç­”: {answer}")
            
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    print()


def demo_rag_chain_official():
    """6.2 ä½¿ç”¨ create_retrieval_chainï¼ˆå®˜æ–¹æ¨èï¼‰"""
    print("=" * 60)
    print("6.2 create_retrieval_chainï¼ˆå®˜æ–¹æ¨èæ–¹å¼ï¼‰")
    print("=" * 60)
    
    try:
        from langchain.chains import create_retrieval_chain
        from langchain.chains.combine_documents import create_stuff_documents_chain
        from langchain_chroma import Chroma
        from langchain_core.documents import Document
        from langchain_core.prompts import ChatPromptTemplate
        
        embeddings = get_embeddings()
        
        # å‡†å¤‡çŸ¥è¯†åº“
        docs = [
            Document(page_content="LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„å¼€æºæ¡†æ¶ã€‚"),
            Document(page_content="RAG é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†æ¥å¢å¼º LLM çš„å›ç­”èƒ½åŠ›ã€‚"),
            Document(page_content="å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡åµŒå…¥ã€‚"),
        ]
        
        vectorstore = Chroma.from_documents(docs, embeddings)
        retriever = vectorstore.as_retriever()
        
        # ç³»ç»Ÿæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä¸çŸ¥é“ã€‚ä¿æŒå›ç­”ç®€æ´ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])
        
        # åˆ›å»ºæ–‡æ¡£å¤„ç†é“¾ï¼ˆstuff = å°†æ‰€æœ‰æ–‡æ¡£å¡å…¥ promptï¼‰
        question_answer_chain = create_stuff_documents_chain(llm, prompt)
        
        # åˆ›å»ºæ£€ç´¢é“¾
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)
        
        # è°ƒç”¨
        response = rag_chain.invoke({"input": "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"})
        
        print(f"é—®é¢˜: ä»€ä¹ˆæ˜¯ RAGï¼Ÿ")
        print(f"å›ç­”: {response['answer']}")
        print(f"\næ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°: {len(response['context'])}")
        for i, doc in enumerate(response['context']):
            print(f"  æ–‡æ¡£ {i+1}: {doc.page_content[:50]}...")
            
    except ImportError as e:
        print(f"éœ€è¦å®‰è£…: {e}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    print()


def demo_rag_with_history():
    """6.3 å¸¦å¯¹è¯å†å²çš„ RAG"""
    print("=" * 60)
    print("6.3 å¸¦å¯¹è¯å†å²çš„ RAG")
    print("=" * 60)
    
    try:
        from langchain.chains import create_history_aware_retriever, create_retrieval_chain
        from langchain.chains.combine_documents import create_stuff_documents_chain
        from langchain_chroma import Chroma
        from langchain_core.documents import Document
        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
        from langchain_core.messages import HumanMessage, AIMessage
        
        embeddings = get_embeddings()
        
        docs = [
            Document(page_content="LangChain æ˜¯ä¸€ä¸ª LLM åº”ç”¨å¼€å‘æ¡†æ¶ï¼Œæ”¯æŒ Python å’Œ JavaScriptã€‚"),
            Document(page_content="LangChain çš„ä¸»è¦ç»„ä»¶åŒ…æ‹¬ï¼šModelsã€Promptsã€Chainsã€Agentsã€Memoryã€‚"),
            Document(page_content="LangGraph æ˜¯ LangChain å›¢é˜Ÿå¼€å‘çš„çŠ¶æ€æœºæ¡†æ¶ï¼Œç”¨äºå¤æ‚ Agentã€‚"),
        ]
        
        vectorstore = Chroma.from_documents(docs, embeddings)
        retriever = vectorstore.as_retriever()
        
        # 1. åˆ›å»ºå†å²æ„ŸçŸ¥æ£€ç´¢å™¨ï¼ˆå°†å¯¹è¯å†å²èå…¥æ£€ç´¢ï¼‰
        contextualize_q_prompt = ChatPromptTemplate.from_messages([
            ("system", "æ ¹æ®å¯¹è¯å†å²ï¼Œå°†ç”¨æˆ·çš„é—®é¢˜æ”¹å†™ä¸ºç‹¬ç«‹çš„é—®é¢˜ã€‚"),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ])
        
        history_aware_retriever = create_history_aware_retriever(
            llm, retriever, contextualize_q_prompt
        )
        
        # 2. åˆ›å»ºé—®ç­”é“¾
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", "åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context}"),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ])
        
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        
        # 3. ç»„åˆæˆå®Œæ•´çš„ RAG é“¾
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
        
        # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
        chat_history = []
        
        # ç¬¬ä¸€è½®
        q1 = "LangChain æ˜¯ä»€ä¹ˆï¼Ÿ"
        r1 = rag_chain.invoke({"input": q1, "chat_history": chat_history})
        print(f"ç”¨æˆ·: {q1}")
        print(f"AI: {r1['answer']}")
        
        chat_history.extend([
            HumanMessage(content=q1),
            AIMessage(content=r1['answer'])
        ])
        
        # ç¬¬äºŒè½®ï¼ˆä½¿ç”¨ä»£è¯"å®ƒ"ï¼Œéœ€è¦ç†è§£ä¸Šä¸‹æ–‡ï¼‰
        q2 = "å®ƒæœ‰å“ªäº›ä¸»è¦ç»„ä»¶ï¼Ÿ"
        r2 = rag_chain.invoke({"input": q2, "chat_history": chat_history})
        print(f"\nç”¨æˆ·: {q2}")
        print(f"AI: {r2['answer']}")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    print()


def demo_rag_streaming():
    """6.4 æµå¼è¾“å‡ºçš„ RAG"""
    print("=" * 60)
    print("6.4 æµå¼è¾“å‡ºçš„ RAG")
    print("=" * 60)
    
    try:
        from langchain_chroma import Chroma
        from langchain_core.documents import Document
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.runnables import RunnablePassthrough
        
        embeddings = get_embeddings()
        
        docs = [
            Document(page_content="LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ LLM åº”ç”¨å¼€å‘æ¡†æ¶ã€‚"),
            Document(page_content="å®ƒæä¾›äº†ä¸°å¯Œçš„ç»„ä»¶æ¥æ„å»ºå¤æ‚çš„ AI åº”ç”¨ã€‚"),
        ]
        
        vectorstore = Chroma.from_documents(docs, embeddings)
        retriever = vectorstore.as_retriever()
        
        prompt = ChatPromptTemplate.from_template("""
åŸºäºä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š""")
        
        def format_docs(docs):
            return "\n".join(doc.page_content for doc in docs)
        
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        print("é—®é¢˜: ä»‹ç»ä¸€ä¸‹ LangChain")
        print("æµå¼å›ç­”: ", end="")
        
        # æµå¼è¾“å‡º
        for chunk in rag_chain.stream("ä»‹ç»ä¸€ä¸‹ LangChain"):
            print(chunk, end="", flush=True)
        print()
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    print()


# ============================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šRAG æœ€ä½³å®è·µ
# ============================================================

def print_best_practices():
    """æ‰“å° RAG æœ€ä½³å®è·µ"""
    print("=" * 60)
    print("ğŸ“Š RAG æœ€ä½³å®è·µ")
    print("=" * 60)
    print("""
    æ–‡æ¡£å¤„ç†æœ€ä½³å®è·µï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. é€‰æ‹©åˆé€‚çš„ chunk_sizeï¼š
       - å¤ªå°ï¼šä¸¢å¤±ä¸Šä¸‹æ–‡
       - å¤ªå¤§ï¼šæ£€ç´¢ä¸ç²¾ç¡®
       - æ¨èï¼š500-1000 å­—ç¬¦ï¼Œæ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´
    
    2. è®¾ç½®é€‚å½“çš„ chunk_overlapï¼š
       - æ¨èï¼šchunk_size çš„ 10-20%
       - ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
    
    3. ä¿ç•™å…ƒæ•°æ®ï¼š
       - æ¥æºã€é¡µç ã€æ—¥æœŸç­‰
       - ä¾¿äºè¿‡æ»¤å’Œæº¯æº
    
    æ£€ç´¢ä¼˜åŒ–ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. é€‰æ‹©åˆé€‚çš„ k å€¼ï¼š
       - k å¤ªå°ï¼šå¯èƒ½é—æ¼ç›¸å…³ä¿¡æ¯
       - k å¤ªå¤§ï¼šå¼•å…¥å™ªéŸ³ï¼Œå¢åŠ æˆæœ¬
       - æ¨èï¼š3-5 ä¸ªæ–‡æ¡£
    
    2. ä½¿ç”¨ MMR å¢åŠ å¤šæ ·æ€§ï¼š
       - é¿å…æ£€ç´¢åˆ°é‡å¤å†…å®¹
       - lambda_mult æ§åˆ¶å¤šæ ·æ€§ç¨‹åº¦
    
    3. ç»“åˆå…ƒæ•°æ®è¿‡æ»¤ï¼š
       - å…ˆè¿‡æ»¤å†æ£€ç´¢ï¼Œæé«˜æ•ˆç‡
       - é€‚åˆæœ‰æ˜ç¡®è¿‡æ»¤æ¡ä»¶çš„åœºæ™¯
    
    Prompt è®¾è®¡ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. æ˜ç¡®æŒ‡ç¤ºåŸºäºä¸Šä¸‹æ–‡å›ç­”
    2. å¤„ç†"ä¸çŸ¥é“"çš„æƒ…å†µ
    3. è¦æ±‚å¼•ç”¨æ¥æºï¼ˆå¦‚æœéœ€è¦ï¼‰
    
    ç”Ÿäº§ç¯å¢ƒå»ºè®®ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ åœºæ™¯                â”‚ æ¨èæ–¹æ¡ˆ                            â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ å¼€å‘/æµ‹è¯•           â”‚ Chromaï¼ˆå†…å­˜æˆ–æœ¬åœ°æŒä¹…åŒ–ï¼‰          â”‚
    â”‚ å°è§„æ¨¡ç”Ÿäº§          â”‚ Chroma/FAISS + æœ¬åœ°æŒä¹…åŒ–           â”‚
    â”‚ å¤§è§„æ¨¡ç”Ÿäº§          â”‚ Pinecone/Milvus/Qdrantï¼ˆäº‘æœåŠ¡ï¼‰    â”‚
    â”‚ ä¼ä¸šçº§              â”‚ è‡ªå»º Milvus/Elasticsearch           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    å¸¸è§é—®é¢˜æ’æŸ¥ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. æ£€ç´¢ç»“æœä¸ç›¸å…³ï¼š
       - æ£€æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦é€‚åˆä½ çš„è¯­è¨€/é¢†åŸŸ
       - è°ƒæ•´ chunk_size
       - å°è¯•ä¸åŒçš„æ£€ç´¢ç­–ç•¥
    
    2. å›ç­”ä¸å‡†ç¡®ï¼š
       - æ£€æŸ¥æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦æ­£ç¡®
       - ä¼˜åŒ– Prompt
       - å¢åŠ  k å€¼
    
    3. æ€§èƒ½é—®é¢˜ï¼š
       - ä½¿ç”¨æ›´é«˜æ•ˆçš„å‘é‡å­˜å‚¨
       - æ·»åŠ ç¼“å­˜
       - å¼‚æ­¥å¤„ç†
    """)


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main():
    print("\nğŸ“š ç¬¬ä¸ƒè¯¾ï¼šRAG åŸºç¡€ (æ£€ç´¢å¢å¼ºç”Ÿæˆ)\n")
    
    print("\n" + "=" * 60)
    print("ğŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡æ¡£åŠ è½½å™¨")
    print("=" * 60)
    demo_document_loaders()
    demo_text_loader()
    demo_web_loader()
    demo_directory_loader()
    
    print("\n" + "=" * 60)
    print("ğŸ“š ç¬¬äºŒéƒ¨åˆ†ï¼šæ–‡æœ¬åˆ†å‰²å™¨")
    print("=" * 60)
    demo_text_splitters()
    demo_split_documents()
    
    print("\n" + "=" * 60)
    print("ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‘é‡åµŒå…¥")
    print("=" * 60)
    demo_embeddings()
    
    print("\n" + "=" * 60)
    print("ğŸ“š ç¬¬å››éƒ¨åˆ†ï¼šå‘é‡å­˜å‚¨")
    print("=" * 60)
    demo_vector_store()
    demo_vector_store_persistence()
    demo_faiss_vector_store()
    
    print("\n" + "=" * 60)
    print("ğŸ“š ç¬¬äº”éƒ¨åˆ†ï¼šæ£€ç´¢å™¨")
    print("=" * 60)
    demo_basic_retriever()
    demo_multi_query_retriever()
    demo_contextual_compression()
    demo_self_query_retriever()
    
    print("\n" + "=" * 60)
    print("ğŸ“š ç¬¬å…­éƒ¨åˆ†ï¼šæ„å»º RAG é“¾")
    print("=" * 60)
    demo_rag_chain_manual()
    demo_rag_chain_official()
    demo_rag_with_history()
    demo_rag_streaming()
    
    print_best_practices()
    
    print("\n" + "=" * 60)
    print("ğŸ“Œ ç¬¬ä¸ƒè¯¾æ€»ç»“")
    print("=" * 60)
    print("""
    RAG å®Œæ•´æµç¨‹
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ç´¢å¼•é˜¶æ®µï¼šæ–‡æ¡£åŠ è½½ â†’ æ–‡æœ¬åˆ†å‰² â†’ å‘é‡åµŒå…¥ â†’ å­˜å…¥å‘é‡åº“
    æ£€ç´¢é˜¶æ®µï¼šç”¨æˆ·é—®é¢˜ â†’ å‘é‡æ£€ç´¢ â†’ æ„å»º Prompt â†’ LLM ç”Ÿæˆ
    
    æ ¸å¿ƒç»„ä»¶
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Document Loaders  : TextLoader, WebBaseLoader, DirectoryLoader
    Text Splitters    : RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰
    Embeddings        : DashScope, OpenAI, HuggingFace
    Vector Stores     : Chromaï¼ˆå¼€å‘ï¼‰, FAISS/Pineconeï¼ˆç”Ÿäº§ï¼‰
    Retrievers        : VectorStoreRetriever, MultiQuery, SelfQuery
    
    RAG é“¾æ„å»ºæ–¹å¼
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    æ‰‹åŠ¨ LCEL é“¾          : å®Œå…¨æ§åˆ¶ï¼Œé€‚åˆå­¦ä¹ å’Œè‡ªå®šä¹‰
    create_retrieval_chain: å®˜æ–¹æ¨èï¼Œè‡ªåŠ¨å¤„ç†æ–‡æ¡£æ ¼å¼
    å¸¦å†å²çš„ RAG          : æ”¯æŒå¤šè½®å¯¹è¯ï¼Œç†è§£ä¸Šä¸‹æ–‡
    
    ä¸‹ä¸€è¯¾é¢„å‘Šï¼šç¬¬å…«è¯¾ Agentsï¼ˆæ™ºèƒ½ä»£ç†ï¼‰
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    å­¦ä¹ å¦‚ä½•æ„å»ºèƒ½å¤Ÿè‡ªä¸»å†³ç­–ã€ä½¿ç”¨å·¥å…·çš„ AI Agent
    """)


if __name__ == "__main__":
    main()
