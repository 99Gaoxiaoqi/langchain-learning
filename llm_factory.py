"""
LLM å·¥å‚æ¨¡å— - ç»Ÿä¸€çš„æ¨¡å‹å…¼å®¹å±‚
æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®åˆ‡æ¢ä¸åŒçš„å¤§æ¨¡å‹æä¾›å•†
"""
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()

# é¢„ç½®çš„æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "models": ["qwen-turbo", "qwen-plus", "qwen-max", "qwen2.5-32b-instruct"],
        "default_model": "qwen2.5-32b-instruct",
        "env_key": "DASHSCOPE_API_KEY",
    },
    "deepseek": {
        "base_url": "https://api.deepseek.com/v1",
        "models": ["deepseek-chat", "deepseek-reasoner"],
        "default_model": "deepseek-chat",
        "env_key": "DEEPSEEK_API_KEY",
    },
    "moonshot": {
        "base_url": "https://api.moonshot.cn/v1",
        "models": ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"],
        "default_model": "moonshot-v1-8k",
        "env_key": "MOONSHOT_API_KEY",
    },
    "zhipu": {
        "base_url": "https://open.bigmodel.cn/api/paas/v4",
        "models": ["glm-4", "glm-4-flash", "glm-4-plus"],
        "default_model": "glm-4-flash",
        "env_key": "ZHIPU_API_KEY",
    },
    "openai": {
        "base_url": "https://api.zhizengzeng.com/v1",
        "models": ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo"],
        "default_model": "gpt-4o-mini",
        "env_key": "OPENAI_API_KEY",
    },
}


def get_llm(
    provider: str = None,
    model: str = None,
    temperature: float = 0.7,
    **kwargs
) -> ChatOpenAI:
    """
    è·å– LLM å®ä¾‹çš„ç»Ÿä¸€å…¥å£
    
    Args:
        provider: æ¨¡å‹æä¾›å•† (qwen/deepseek/moonshot/zhipu/openai)
                  é»˜è®¤ä» LLM_PROVIDER ç¯å¢ƒå˜é‡è¯»å–ï¼Œæœªè®¾ç½®åˆ™ç”¨ qwen
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨è¯¥æä¾›å•†çš„é»˜è®¤æ¨¡å‹
        temperature: æ¸©åº¦å‚æ•°
        **kwargs: å…¶ä»– ChatOpenAI æ”¯æŒçš„å‚æ•°
    
    Returns:
        ChatOpenAI å®ä¾‹
    
    Example:
        # ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆç¯å¢ƒå˜é‡ï¼‰
        llm = get_llm()
        
        # æŒ‡å®šæä¾›å•†
        llm = get_llm(provider="deepseek")
        
        # æŒ‡å®šæä¾›å•†å’Œæ¨¡å‹
        llm = get_llm(provider="qwen", model="qwen-max")
    """
    # ç¡®å®šæä¾›å•†
    provider = provider or os.getenv("LLM_PROVIDER", "qwen")
    provider = provider.lower()
    
    if provider not in MODEL_CONFIGS:
        raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}ï¼Œå¯é€‰: {list(MODEL_CONFIGS.keys())}")
    
    config = MODEL_CONFIGS[provider]
    
    # è·å– API Key
    api_key = os.getenv(config["env_key"])
    if not api_key:
        raise ValueError(f"è¯·è®¾ç½®ç¯å¢ƒå˜é‡ {config['env_key']}")
    
    # ç¡®å®šæ¨¡å‹
    model = model or config["default_model"]
    
    return ChatOpenAI(
        model=model,
        base_url=config["base_url"],
        api_key=api_key,
        temperature=temperature,
        **kwargs
    )


def list_providers():
    """åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æä¾›å•†åŠå…¶æ¨¡å‹"""
    print("æ”¯æŒçš„æ¨¡å‹æä¾›å•†ï¼š")
    print("-" * 50)
    for name, config in MODEL_CONFIGS.items():
        print(f"\nğŸ“¦ {name}")
        print(f"   ç¯å¢ƒå˜é‡: {config['env_key']}")
        print(f"   é»˜è®¤æ¨¡å‹: {config['default_model']}")
        print(f"   å¯ç”¨æ¨¡å‹: {', '.join(config['models'])}")


if __name__ == "__main__":
    list_providers()
